{
  "hash": "f6e51f45e9ca669a3453ee0c98b93aa5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Character Encoding\"\ndescription: \"Introduction to character encoding\"\nimage: character-encoding-img/encoding.jpg\ncategories:\n    - \"Computer Science\"\ndate: 2025-11-11\nformat: html\nfreeze: true\n---\n\nThe first time I dug deeper into this topic was when I encountered a program that would\nconsistently print weird characters when reading data from a file.\nIn effect, this was pretty similar to opening a UTF-8 encoded file in Microsoft Excel by double clicking.\nIt will not detect the encoding correctly (assume e.g. cp1252) if the special characters appear sparingly and late in the file.\nAnyway, that was an interesting rabbit hole of encodings and byte-order marks to explore.\nI hope you enjoy this short article.\n\n\n## Everything is Numbers\n\nComputers work with **bits** (binary digits) - zeroes and ones - how can they work with text?\n\nWell, it's quite simple - computers know how to represent numbers.\nSo we'll just assign a number to each character.\nThis process is called 'encoding'.\n\nIs it quite so simple?\nWell no, there's a bit more to it.\nLet's find out.\n\n\n## ASCII\n\nThe most influential character encoding might be ASCII (American Standard Code for Information Interchange, pronounced 'asky').\nThe ASCII encoding is a 7-bit encoding i.e. it maps the numbers from 0 to 127 ($2^7-1$) to a character.\n\n::: {#10900fd3 .cell execution_count=1}\n``` {.python .cell-code}\nprint(f\"decimal - bit-pattern - character\")\nfor i in range(89, 100):\n    print(f\"{i:^7} - {format(i, '07b'):^11} - {chr(i):^9}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ndecimal - bit-pattern - character\n  89    -   1011001   -     Y    \n  90    -   1011010   -     Z    \n  91    -   1011011   -     [    \n  92    -   1011100   -     \\    \n  93    -   1011101   -     ]    \n  94    -   1011110   -     ^    \n  95    -   1011111   -     _    \n  96    -   1100000   -     `    \n  97    -   1100001   -     a    \n  98    -   1100010   -     b    \n  99    -   1100011   -     c    \n```\n:::\n:::\n\n\nThe ASCII encoding contains all characters used in the English language and additional _control_ characters such as e.g. carriage return.\nNowadays, using 7-bits seems a bit odd, but this happened before a byte was standardized to 8-bits.\nToday the ASCII encoding looks the same, just with an additional '0' prefixed to each bit-pattern.\n\n\n## Extended ASCII Encodings\n\nAs most computers work with an 8-bit byte, people started to use the additional bit\n(=> 128 additional bit-patterns) to encode more characters.\nIn western Europe this resulted in encodings like Windows-1252 or Latin-1 to represent letters such as Ãœ (`11011100` in Latin-1).\nIn other parts of the world, other language-specific encodings arose (see for example\n[TSCII](https://en.wikipedia.org/wiki/Tamil_Script_Code_for_Information_Interchange) etc.).\nThis lead to a confusing situation where the same bit-pattern could stand for very different things.\n\n::: {#88ff56b4 .cell execution_count=2}\n``` {.python .cell-code}\nchar_as_byte = \"Ãœ\".encode(encoding=\"latin_1\")\nchar_as_int = int.from_bytes(char_as_byte, byteorder=\"little\")\n# note: byteorder does not matter for this encoding\nprint(f\"Ãœ is {char_as_int:08b}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nÃœ is 11011100\n```\n:::\n:::\n\n\n## Unicode\n\nUnicode is a universal encoding that supports all languages, symbols and even emojis.\nIt maps each character to an abstract _unicode code point_ often denoted `U+<hexadeximal number>`.\n\n::: {#71df7773 .cell execution_count=3}\n``` {.python .cell-code}\ncharacters = ['A', 'Ãœ', 'ðŸ˜†']\nprint(\"code point - character\")\nfor char in characters:\n    cp = f\"U+{ord(char):X}\"\n    print(f'{cp:^10}{char:^9}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ncode point - character\n   U+41       A    \n   U+DC       Ãœ    \n U+1F606      ðŸ˜†    \n```\n:::\n:::\n\n\nSo why do I call it an _abstract_ code point?\nWhile the code point is a number, it does not specify how many bits are used to represent the actual characters.\nThis is done by the Unicode Transformation Format (**UTF**).\nThere are three commonly used formats: UTF-8, UTF-16 and UTF-32.\nThe number signifies the minimal number of bits that is used to represent a character.\nThese formats are 'multi-byte' formats i.e. they may use more than one byte to represent a character.\nUTF-32 always uses 4 bytes, UTF-16 uses at least 2 bytes but up to four and UTF-8 uses at least one byte but up to four.\nUTF-32 is pretty simple to understand as it always uses 4 bytes to represent a character.\nThe only issue arising here is byte order, which we'll talk about later.\n\n\n### UTF-8\n\nThe most commonly used Unicode Transformation Format is **UTF-8**,\nwhich uses at least 8 bits and is compatible with ASCII in the lower 128 characters\n(this is how I got away using regular Python strings which are UTF-8 encoded in the ASCII section).\n\nIt works by encoding the number of following bytes in the first byte (start byte).\nIf a byte starts with:\n\n- `0` is not followed by any more bytes => ASCII character\n- `110` followed by one more byte\n- `1110` followed by two more bytes\n- `11110` followed by three more bytes\n\nThe follow-bytes start with `10`.\n\nThe 'payload' in an UTF-8 encoded character is everything except the start and follow byte prefixes.\nLet's look at some examples:\n\nThe Character 'A' would be <span style=\"color:blue\">0</span>1000001 (as in ASCII).\nPayload = 100 0001 = U+41.\n\nThe Character 'Ãœ' is encoded as\n<span style=\"color:blue\">110</span>00011\n<span style=\"color:blue\">10</span>011100\n\nPayload = 1101 1100 = U+DC.\n\nThe emoji ðŸ˜… is encoded as\n<span style=\"color:blue\">11110</span>000\n<span style=\"color:blue\">10</span>011111\n<span style=\"color:blue\">10</span>011000\n<span style=\"color:blue\">10</span>000101\n\nPayload = 0001 1111 0110 0000 0101 = U+1F606.\n\n\n### Byte-Order\n\nOne (last?) complication arises when using multiple bytes to represent a character:\nWhich byte should be the first byte (in a message, in storage)?\nComputers and protocols use different byte-orders - so-called endianness.\n\nIn a little-endian system the character 'Ãœ' in UTF-8 encoding would be represented as\n`1100 1101`, while it would be represented as `1101 1100` in a big-endian system.\n(In my opinion the terms are a bit confusing, imho it should be call 'little-startian' \nbecause we start with the lowest-value byte.)\n\nSome programs add a byte-order mark (**BOM**) to the start of their files.\nThe mark is `0xFEFF`.\nShould a program receive a BOM as `FF` and `FE` it would know that the bytes are transmitted in little-endian order.\nHowever, this is not recommended for UTF-8 encoded files, where it can be detected without BOM anyway.\n\n\n## Summary\n\nSo to sum up, an encoding is a mapping of characters to numbers (the reverse is decoding).\nUnicode is a universal encoding that has significantly simplifed working\nwith character encodings with UTF-8 becoming the standard format on the web.\nKnowing the encoding of a file is crucial for making sense of its contents.\n\nOne final thing I learned about Unicode is that there actually are undefined regions\nin the standard, reserved for private use [Private Use Areas](https://en.wikipedia.org/wiki/Private_Use_Areas).\nSo, not entirely unversal after all...\nI guess things stay interesting!\n\nWe did not go into any other topics regarding text representation on computers.\nC's NULL-terminated strings, Pascals fixed-length strings and other concepts like compression using Huffman coding etc.\nMaybe we'll get there another time.\n\n",
    "supporting": [
      "character-encoding_files"
    ],
    "filters": [],
    "includes": {}
  }
}